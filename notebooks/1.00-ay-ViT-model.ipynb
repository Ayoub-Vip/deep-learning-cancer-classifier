{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from vit_pytorch import ViT\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tdata\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mayoub-dev2000\u001b[0m (\u001b[33mayoub-dev2000-university-of-liege\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ayoubvip/deep-learning-cancer-classifier/notebooks/wandb/run-20250516_184709-n65c479r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ayoub-dev2000-university-of-liege/cancer-classifier-ViT-model-DL-course/runs/n65c479r' target=\"_blank\">charmed-paper-1</a></strong> to <a href='https://wandb.ai/ayoub-dev2000-university-of-liege/cancer-classifier-ViT-model-DL-course' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ayoub-dev2000-university-of-liege/cancer-classifier-ViT-model-DL-course' target=\"_blank\">https://wandb.ai/ayoub-dev2000-university-of-liege/cancer-classifier-ViT-model-DL-course</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ayoub-dev2000-university-of-liege/cancer-classifier-ViT-model-DL-course/runs/n65c479r' target=\"_blank\">https://wandb.ai/ayoub-dev2000-university-of-liege/cancer-classifier-ViT-model-DL-course/runs/n65c479r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-16 18:47:18.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcancer_classifier.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /home/ayoubvip/deep-learning-cancer-classifier\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "run = wandb.init(\n",
    "    project='cancer-classifier-ViT-model-DL-course',\n",
    "    config={}\n",
    "    )\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from cancer_classifier.config import MODELS_DIR, RAW_DATA_DIR, PROCESSED_DATA_DIR, INTERIM_DATA_DIR, CLASSES\n",
    "from cancer_classifier.processing.image_utils import adjust_image_contrast, resize_image_tensor, normalize_image_tensor, augment_image_tensor, process_dataset, save_processed_images, crop_image\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (256, 256)\n",
    "clip_limit = 2.0\n",
    "tile_size = (1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data crop and contract equilazing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, cls in enumerate(CLASSES):\n",
    "#     cls_dir = os.path.join(RAW_DATA_DIR, cls)\n",
    "#     for img_name in os.listdir(cls_dir):\n",
    "#         img_path = os.path.join(cls_dir, img_name)\n",
    "#         img = crop_image(img_path, img_size, clip_limit, tile_size)\n",
    "#         # img = adjust_image_contrast(img)\n",
    "\n",
    "#         # save in processed directory\n",
    "#         cv2.imwrite(os.path.join(PROCESSED_DATA_DIR, cls, img_name), img)\n",
    "#         # save_processed_images(img_tensor, os.path.join(PROCESSED_DATA_DIR, cls), img_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "    torchvision.transforms.Resize(size=img_size),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomVerticalFlip(),\n",
    "    torchvision.transforms.RandomRotation(10),\n",
    "    torchvision.transforms.RandomResizedCrop(size=img_size, scale=(0.8, 1.0)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(root=PROCESSED_DATA_DIR, transform=transformers, target_transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data spliting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_nbr = 127\n",
    "train_ratio = 0.80\n",
    "test_ratio = 0.10\n",
    "val_ratio = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset class distribution: {'2': 1640, '1': 1601, '0': 1604}\n",
      "Validation dataset class distribution: {'1': 211, '2': 209, '0': 186}\n",
      "Test dataset class distribution: {'0': 214, '2': 199, '1': 192}\n"
     ]
    }
   ],
   "source": [
    "rand_gen = torch.Generator().manual_seed(142)\n",
    "train_dataset, val_dataset, test_dataset = tdata.random_split(\n",
    "    dataset = dataset,\n",
    "    lengths=[train_ratio, val_ratio, test_ratio],\n",
    "    generator=rand_gen\n",
    ")\n",
    "\n",
    "train_loader = tdata.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_nbr,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = tdata.DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_nbr,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = tdata.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_nbr,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "def get_class_distribution(dataset):\n",
    "    class_distribution = {}\n",
    "    for _, label in dataset:\n",
    "        if label.__str__() not in class_distribution:\n",
    "            class_distribution[label.__str__()] = 0\n",
    "        class_distribution[label.__str__()] += 1\n",
    "    return class_distribution\n",
    "\n",
    "print(\"Train dataset class distribution:\", get_class_distribution(train_dataset))\n",
    "print(\"Validation dataset class distribution:\", get_class_distribution(val_dataset))\n",
    "print(\"Test dataset class distribution:\", get_class_distribution(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture: Vision Transformer (Dosovitskiy et al., 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=256, patch_size=32, num_hiddens=512):\n",
    "        super().__init__()\n",
    "        def _make_tuple(x):\n",
    "            if not isinstance(x, (list, tuple)):\n",
    "                return (x, x)\n",
    "            return x\n",
    "        img_size, patch_size = _make_tuple(img_size), _make_tuple(patch_size)\n",
    "        self.num_patches = (img_size[0] // patch_size[0]) * (\n",
    "            img_size[1] // patch_size[1])\n",
    "        self.conv = nn.LazyConv2d(num_hiddens, kernel_size=patch_size,\n",
    "                                  stride=patch_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Output shape: (batch size, no. of patches, no. of channels)\n",
    "        return self.conv(X).flatten(2).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTMLP(nn.Module):\n",
    "    def __init__(self, mlp_num_hiddens, mlp_num_outputs, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.LazyLinear(mlp_num_hiddens)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dense2 = nn.LazyLinear(mlp_num_outputs)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout2(self.dense2(self.dropout1(self.gelu(\n",
    "            self.dense1(x)))))\n",
    "        \n",
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
    "    # X: 3D tensor, valid_lens: 1D or 2D tensor\n",
    "    def _sequence_mask(X, valid_len, value=0):\n",
    "        maxlen = X.size(1)\n",
    "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                            device=X.device)[None, :] < valid_len[:, None]\n",
    "        X[~mask] = value\n",
    "        return X\n",
    "\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # On the last axis, replace masked elements with a very large negative\n",
    "        # value, whose exponentiation outputs 0\n",
    "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
    "    \n",
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled dot product attention.\"\"\"\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # Shape of queries: (batch_size, no. of queries, d)\n",
    "    # Shape of keys: (batch_size, no. of key-value pairs, d)\n",
    "    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n",
    "    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        # Swap the last two dimensions of keys with keys.transpose(1, 2)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / np.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention.\"\"\"\n",
    "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        # Shape of queries, keys, or values:\n",
    "        # (batch_size, no. of queries or key-value pairs, num_hiddens)\n",
    "        # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "        # After transposing, shape of output queries, keys, or values:\n",
    "        # (batch_size * num_heads, no. of queries or key-value pairs,\n",
    "        # num_hiddens / num_heads)\n",
    "        queries = self.W_q(queries).T\n",
    "        keys = self.W_k(keys).T\n",
    "        values = self.W_v(values).T\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # On axis 0, copy the first item (scalar or vector) for num_heads\n",
    "            # times, then copy the next item, and so on\n",
    "            valid_lens = torch.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        # Shape of output: (batch_size * num_heads, no. of queries,\n",
    "        # num_hiddens / num_heads)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        # Shape of output_concat: (batch_size, no. of queries, num_hiddens)\n",
    "        output_concat = output.T\n",
    "        return self.W_o(output_concat)\n",
    "        \n",
    "class ViTBlock(nn.Module):\n",
    "    def __init__(self, num_hiddens, norm_shape, mlp_num_hiddens,\n",
    "                 num_heads, dropout, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(norm_shape)\n",
    "        self.attention = MultiHeadAttention(num_hiddens, num_heads,\n",
    "                                                dropout, use_bias)\n",
    "        self.ln2 = nn.LayerNorm(norm_shape)\n",
    "        self.mlp = ViTMLP(mlp_num_hiddens, num_hiddens, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens=None):\n",
    "        X = X + self.attention(*([self.ln1(X)] * 3), valid_lens)\n",
    "        return X + self.mlp(self.ln2(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTModel(nn.Module):\n",
    "    \"\"\"Vision Transformer.\"\"\"\n",
    "    def __init__(self, img_size, patch_size, num_hiddens, mlp_num_hiddens,\n",
    "                 num_heads, num_blks, emb_dropout, blk_dropout, lr=0.1,\n",
    "                 use_bias=False, num_classes=10):\n",
    "        super().__init__()\n",
    "        # self.save_hyperparameters()\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            img_size, patch_size, num_hiddens)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, num_hiddens))\n",
    "        num_steps = self.patch_embedding.num_patches + 1  # Add the cls token\n",
    "        # Positional embeddings are learnable\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, num_steps, num_hiddens))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(f\"{i}\", ViTBlock(\n",
    "                num_hiddens, num_hiddens, mlp_num_hiddens,\n",
    "                num_heads, blk_dropout, use_bias))\n",
    "        self.head = nn.Sequential(nn.LayerNorm(num_hiddens),\n",
    "                                  nn.Linear(num_hiddens, num_classes))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.patch_embedding(X)\n",
    "        X = torch.cat((self.cls_token.expand(X.shape[0], -1, -1), X), 1)\n",
    "        X = self.dropout(X + self.pos_embedding)\n",
    "        for blk in self.blks:\n",
    "            X = blk(X)\n",
    "        return self.head(X[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16\n",
    "num_hiddens = 516\n",
    "mlp_num_hiddens = 156\n",
    "num_heads = 8\n",
    "num_blks = 2\n",
    "emb_dropout = 0.0001\n",
    "blk_dropout = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vit_model = ViTModel(img_size, patch_size, num_hiddens, mlp_num_hiddens,\n",
    "                 num_heads, num_blks, emb_dropout, blk_dropout, lr=0.1,\n",
    "                 use_bias=False, num_classes=len(CLASSES)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining Loss function, Optimization method, and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "weights_decay = 0.0001\n",
    "learning_rate = 0.0001\n",
    "epochs = 5\n",
    "\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     Vit_model.parameters(),\n",
    "#     lr=learning_rate,\n",
    "#     weight_decay=weights_decay\n",
    "# ) #->70%\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    Vit_model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weights_decay\n",
    ") #->77.4\n",
    "\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     Vit_model.parameters(),\n",
    "#     lr=learning_rate,\n",
    "#     weight_decay=weights_decay\n",
    "# ) #-> 77.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn, epoch=0):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            true_labels.extend(y.cpu().numpy())\n",
    "            pred_labels.extend(pred.argmax(1).cpu().numpy())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    wandb.log({\"epoch\": epoch,\"accuracy\": f\"{(100*correct):>0.1f}\", \"test_loss\": test_loss})\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return true_labels, pred_labels, correct, test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- EPOCH 0------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2045031/2868336285.py:72: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/TensorShape.cpp:3683.)\n",
      "  queries = self.W_q(queries).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.103517  [  127/ 4845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 20%|██        | 1/5 [00:10<00:40, 10.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 0.881669 \n",
      "\n",
      "----------------- EPOCH 1------------------\n",
      "loss: 0.875471  [  127/ 4845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 40%|████      | 2/5 [00:17<00:24,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.803235 \n",
      "\n",
      "----------------- EPOCH 2------------------\n",
      "loss: 0.848864  [  127/ 4845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 60%|██████    | 3/5 [00:23<00:15,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.760624 \n",
      "\n",
      "----------------- EPOCH 3------------------\n",
      "loss: 0.782124  [  127/ 4845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 80%|████████  | 4/5 [00:30<00:07,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.738104 \n",
      "\n",
      "----------------- EPOCH 4------------------\n",
      "loss: 0.828504  [  127/ 4845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:37<00:00,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.747415 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from cancer_classifier.modeling.train import train\n",
    "# import evaluate from cancer_classifier.modeling.evaluate as evaluate\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X = X.to(device)\n",
    "        y = y.to(device) \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_nbr + len(X)\n",
    "            wandb.log({\"step\":  f\"{current:>5d}/{size:>5d}\", \"train_loss\": loss})\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "            \n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(\"----------------- EPOCH \" + str(epoch) + \"------------------\")\n",
    "    train_loop(\n",
    "        dataloader=train_loader,\n",
    "        model=Vit_model,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        epoch=epoch\n",
    "        )\n",
    "    true_labels, pred_labels, correct, test_loss =  test_loop(\n",
    "        dataloader=test_loader,\n",
    "        model=Vit_model,\n",
    "        loss_fn=loss_fn,\n",
    "        epoch=epoch\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.724880 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels, pred_labels, correct, test_loss =  test_loop(\n",
    "    dataloader=test_loader,\n",
    "    model=Vit_model,\n",
    "    loss_fn=loss_fn\n",
    ")\n",
    "# train(\n",
    "#     model=Vit_model,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     test_loader=test_loader,\n",
    "#     loss_fn=loss_fn,\n",
    "#     optimizer=optimizer,\n",
    "#     epochs=epochs,\n",
    "#     device=device\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cancer_classifier.plots import visualize_sample_images,  plot_confusion_matrix\n",
    "import time\n",
    "\n",
    "visualize_sample_images(\n",
    "    dataset=dataset,\n",
    ")\n",
    "plot_confusion_matrix(\n",
    "    true_classes=true_labels,\n",
    "    predicted_classes=pred_labels,\n",
    "    model_name =  time.strftime(\"%Y-%m-%d_%H-%M-%S\") + \"_Vit_model_\" + f\"{(100*correct):>0.1f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### serializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "model_name = f\"{(100*correct):>0.1f}%--\" + time.strftime(\"%Y-%m-%d_%H-%M-%S\") + \"_Vit_model.pth\"\n",
    "torch.save(Vit_model.state_dict(), MODELS_DIR / model_name)\n",
    "print(\"Saved PyTorch Model State to \" +  model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html\n",
    "- https://github.com/lucidrains/vit-pytorch\n",
    "- https://docs.pytorch.org/tutorials/beginner/basics/transforms_tutorial.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
