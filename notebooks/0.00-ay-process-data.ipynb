{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eec417f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tdata\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d008eb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 23:01:38.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcancer_classifier.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /home/ayoubvip/deep-learning-cancer-classifier\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from cancer_classifier.config import MODELS_DIR, RAW_DATA_DIR, PROCESSED_DATA_DIR, INTERIM_DATA_DIR, CLASSES\n",
    "from cancer_classifier.processing.image_utils import adjust_image_contrast, resize_image_tensor, normalize_image_tensor, augment_image_tensor, process_dataset, save_processed_images, crop_image\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aace3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5198dcea",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b1bfd",
   "metadata": {},
   "source": [
    "### data crop and contract equilazing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19ff5f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, cls in enumerate(CLASSES):\n",
    "    cls_dir = os.path.join(RAW_DATA_DIR, cls)\n",
    "    for img_name in os.listdir(cls_dir):\n",
    "        img_path = os.path.join(cls_dir, img_name)\n",
    "        img = crop_image(img_path)\n",
    "        # img = adjust_image_contrast(img)\n",
    "\n",
    "        # save in processed directory\n",
    "        cv2.imwrite(os.path.join(INTERIM_DATA_DIR, cls, img_name), img)\n",
    "        # save_processed_images(img_tensor, os.path.join(PROCESSED_DATA_DIR, cls), img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "693adbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (256, 256)\n",
    "batch_nbr = 16\n",
    "train_ratio = 0.80\n",
    "test_ratio = 0.10\n",
    "val_ratio = 0.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36468fd1",
   "metadata": {},
   "source": [
    "### data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(size=img_size),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomVerticalFlip(),\n",
    "    torchvision.transforms.RandomRotation(10),\n",
    "    # torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    torchvision.transforms.RandomResizedCrop(size=img_size, scale=(0.8, 1.0)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(root=RAW_DATA_DIR, transform=transformers, target_transform=PROCESSED_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a5f21d",
   "metadata": {},
   "source": [
    "### data spliting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62f55156",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_gen = torch.Generator().manual_seed(142)\n",
    "train_dataset, val_dataset, test_dataset = tdata.random_split(\n",
    "    dataset = dataset,\n",
    "    lengths=[train_ratio, val_ratio, test_ratio],\n",
    "    generator=rand_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d2898a",
   "metadata": {},
   "source": [
    "# Model Architecture: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81237981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1, pool_kernel_size=2, pool_stride=2, dropout_prob=0.2):\n",
    "  return nn.Sequential(\n",
    "      nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "      nn.BatchNorm2d(out_channels),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Dropout2d(p=dropout_prob),\n",
    "      nn.MaxPool2d(pool_kernel_size, pool_stride)\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
