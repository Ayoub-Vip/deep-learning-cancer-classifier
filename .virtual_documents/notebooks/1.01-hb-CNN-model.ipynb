import os
import sys
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as tdata
import numpy as np
from tqdm import tqdm
import cv2
import wandb


wandb.login()
run = wandb.init(
    project='cancer-classifier-CNN-model-DL-course',
    config={}
    )

sys.path.append(os.path.abspath(os.path.join(os.getcwd(), "..")))

from cancer_classifier.config import MODELS_DIR, RAW_DATA_DIR, PROCESSED_DATA_DIR, INTERIM_DATA_DIR, CLASSES
from cancer_classifier.processing.image_utils import adjust_image_contrast, resize_image_tensor, normalize_image_tensor, augment_image_tensor, process_dataset, save_processed_images, crop_image

%load_ext autoreload
%autoreload 2


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else "cpu"
print(f"Using device: {device}")








for i, cls in enumerate(CLASSES):
    cls_dir = os.path.join(RAW_DATA_DIR, cls)
    for img_name in os.listdir(cls_dir):
        img_path = os.path.join(cls_dir, img_name)
        img = crop_image(img_path)
        # img = adjust_image_contrast(img)

        # save in processed directory
        cv2.imwrite(os.path.join(INTERIM_DATA_DIR, cls, img_name), img)
        # save_processed_images(img_tensor, os.path.join(PROCESSED_DATA_DIR, cls), img_name)


img_size = (256, 256)
batch_nbr = 16
train_ratio = 0.80
test_ratio = 0.10
val_ratio = 0.10





transformers = torchvision.transforms.Compose([
    torchvision.transforms.Grayscale(num_output_channels=1),
    torchvision.transforms.Resize(size=img_size),
    torchvision.transforms.RandomHorizontalFlip(),
    torchvision.transforms.RandomVerticalFlip(),
    torchvision.transforms.RandomRotation(10),
    # torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    torchvision.transforms.RandomResizedCrop(size=img_size, scale=(0.8, 1.0)),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=[0.5], std=[0.5])
])

dataset = torchvision.datasets.ImageFolder(root=RAW_DATA_DIR, transform=transformers, target_transform=None)





rand_gen = torch.Generator().manual_seed(142)
train_dataset, val_dataset, test_dataset = tdata.random_split(
    dataset = dataset,
    lengths=[train_ratio, val_ratio, test_ratio],
    generator=rand_gen
)

train_loader = tdata.DataLoader(
    dataset=train_dataset,
    batch_size=batch_nbr,
    shuffle=True,
    num_workers=2,
    pin_memory=True
)
val_loader = tdata.DataLoader(
    dataset=val_dataset,
    batch_size=batch_nbr,
    shuffle=False,
    num_workers=2,
    pin_memory=True
)
test_loader = tdata.DataLoader(
    dataset=test_dataset,
    batch_size=batch_nbr,
    shuffle=False,
    num_workers=2,
    pin_memory=True
)
def get_class_distribution(dataset):
    class_distribution = {}
    for _, label in dataset:
        if label.__str__() not in class_distribution:
            class_distribution[label.__str__()] = 0
        class_distribution[label.__str__()] += 1
    return class_distribution

print("Train dataset class distribution:", get_class_distribution(train_dataset))
print("Validation dataset class distribution:", get_class_distribution(val_dataset))
print("Test dataset class distribution:", get_class_distribution(test_dataset))





def conv_block(in_channels, out_channels, kernel_size=3,
                stride=1, padding=1, pool_kernel_size=2,
                pool_stride=2, dropout_prob=0.2):
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),
        nn.BatchNorm2d(out_channels),
        nn.ReLU(inplace=True),
        nn.Dropout2d(p=dropout_prob),
        nn.MaxPool2d(pool_kernel_size, pool_stride)
    )


class CNNModel(nn.Module):
  def __init__(self, num_classes=3):
    super(CNNModel, self).__init__()

    self.conv_block1 = conv_block(1, 64, 3)
    self.conv_block2 = conv_block(64, 128, 5)
    self.conv_block3 = conv_block(128, 256, 5)
    self.conv_block4 = conv_block(256, 512, 3, 2)

    dummy_input = torch.randn(1, 1, 256, 256) # Sadece bir görsel için çıkış boyutu hesaplar
    dummy_output = self.conv_block4(self.conv_block3(self.conv_block2(self.conv_block1(dummy_input))))

    flattened_size = torch.flatten(dummy_output, 1).size(1)

    print(f'the full architecture parameters size: {flattened_size}')

    self.fc_layers = nn.Sequential(
           nn.Dropout(0.3),
           nn.Linear(flattened_size, 512), # Hesaplanan boyutu kullan
           nn.ReLU(inplace=True),
           nn.Linear(512, num_classes)
        )

  def forward(self, x):
    x = self.conv_block1(x)
    x = self.conv_block2(x)
    x = self.conv_block3(x)
    x = self.conv_block4(x)

    x = torch.flatten(x, 1)

    x = self.fc_layers(x)

    return x



CNN_model = CNNModel(num_classes=len(CLASSES)).to(device)
# print(CNN_model)


import numpy as np
import matplotlib.pyplot as plt

def display_activations_side_by_side(activations, num_columns=6): #j'ai mis 6 car dans le cours on affiche 6 on peut changer
    """
    Displays the first `num_columns` feature-maps of a tensor `activations`
    
    activations : torch.Tensor de shape [1, n_filters, H, W]
    num_columns : nombre de feature-maps à afficher max
    """
    
    n_filters, H, W = activations.shape[1], activations.shape[2], activations.shape[3]
    n_cols = min(num_columns, n_filters)
    
    fig, axes = plt.subplots(1, n_cols, figsize=(n_cols*2, 2))
    
    for i in range(n_cols):
        fmap = activations[0, i].detach().cpu().numpy()
        ax = axes[i] if n_cols > 1 else axes
        ax.imshow(fmap, cmap='viridis')
        ax.set_title(f'FM {i}')
        ax.axis('off')
    
    plt.tight_layout()
    plt.show()

    
def get_activation(model, block_idx, x):
    """Renvoie la sortie du conv_block #block_idx (0-based)."""
    blocks = [
        model.conv_block1,
        model.conv_block2,
        model.conv_block3,
        model.conv_block4
    ]
    out = x
    with torch.no_grad():
        for i in range(block_idx + 1):
            out = blocks[i](out)
    return out



sample_img, _ = dataset[0]
x = sample_img.unsqueeze(0).to(device)

for idx in range(4):
    acts = get_activation(CNN_model, idx, x)
    print(f"Après conv_block{idx+1} :")
    display_activations_side_by_side(acts, num_columns=8)









loss_fn = nn.CrossEntropyLoss()
weights_decay = 0.0001
learning_rate = 0.0001
epochs = 5

# optimizer = torch.optim.Adam(
#     CNN_model.parameters(),
#     lr=learning_rate,
#     weight_decay=weights_decay
# ) #->70%

optimizer = torch.optim.AdamW(
    CNN_model.parameters(),
    lr=learning_rate,
    weight_decay=weights_decay
) #->77.4

# optimizer = torch.optim.SGD(
#     CNN_model.parameters(),
#     lr=learning_rate,
#     weight_decay=weights_decay
# ) #> 77.5


from cancer_classifier.modeling.train import train
# import evaluate from cancer_classifier.modeling.evaluate as evaluate
def train_loop(dataloader, model, loss_fn, optimizer, epoch):
    running_loss = 0.0
    running_corrects = 0
    size = len(dataloader.dataset)
    # Set the model to training mode - important for batch normalization and dropout layers
    # Unnecessary in this situation but added for best practices
    model.train()
    for batch, (X, y) in enumerate(dataloader):
        # Compute prediction and loss
        X = X.to(device)
        y = y.to(device) 
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        # accumulation
        running_loss += loss.item() * X.size(0)
        running_corrects += (pred.argmax(1) == y).sum().item()

        if batch % 100 == 0:
            loss, current = loss.item(), batch * batch_nbr + len(X)
            wandb.log({"step":  f"{current:>5d}/{size:>5d}", "train_loss": loss})
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")
    epoch_loss = running_loss / size
    epoch_acc  = running_corrects / size
    return epoch_loss, epoch_acc
            

for epoch in tqdm(range(epochs)):
    history = {
    'train_loss': [],
    'val_loss': [],
    'train_acc': [],   # si tu veux track l’exactitude à l’époque
    'val_acc': []
    }

    print("----------------- EPOCH " + str(epoch) + "------------------")
    train_loss, train_acc = train_loop(
        dataloader=train_loader,
        model=CNN_model,
        loss_fn=loss_fn,
        optimizer=optimizer,
        epoch=epoch
        )
    
    history['train_loss'].append(train_loss)
    history['train_acc'].append(train_acc)

    true_labels, pred_labels, correct, test_loss =  test_loop(
        dataloader=test_loader,
        model=CNN_model,
        loss_fn=loss_fn,
        epoch=epoch
    )

    val_acc = correct / len(test_loader.dataset)
    history['val_loss'].append(test_loss)
    history['val_acc'].append(val_acc)

    print(f"Epoch {epoch} → "
            f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | "
            f"Val Loss:   {test_loss:.4f}, Val Acc:   {val_acc:.4f}")



def test_loop(dataloader, model, loss_fn, epoch=0):
    # Set the model to evaluation mode - important for batch normalization and dropout layers
    # Unnecessary in this situation but added for best practices
    model.eval()
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    test_loss, correct = 0, 0
    true_labels = []
    pred_labels = []
    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode
    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True
    with torch.no_grad():
        for X, y in dataloader:
            X = X.to(device)
            y = y.to(device)
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()
            true_labels.extend(y.cpu().numpy())
            pred_labels.extend(pred.argmax(1).cpu().numpy())

    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
    wandb.log({"epoch": epoch,"accuracy": f"{(100*correct):>0.1f}", "test_loss": test_loss})


    
    
    return true_labels, pred_labels, correct, test_loss


true_labels, pred_labels, correct, test_loss =  test_loop(
    dataloader=test_loader,
    model=CNN_model,
    loss_fn=loss_fn
)


import matplotlib.pyplot as plt

# Perte
plt.figure(figsize=(8,5))
plt.plot(history['train_loss'], label='Train Loss')
plt.plot(history['val_loss'],   label='Val Loss')
plt.title('Training vs Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Exactitude
plt.figure(figsize=(8,5))
plt.plot(history['train_acc'], label='Train Acc')
plt.plot(history['val_acc'],   label='Val Acc')
plt.title('Training vs Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()









from cancer_classifier.plots import visualize_sample_images,  plot_confusion_matrix
import time

visualize_sample_images(
    dataset=dataset,
)
plot_confusion_matrix(
    true_classes=true_labels,
    predicted_classes=pred_labels,
    model_name =  time.strftime("%Y-%m-%d_%H-%M-%S") + "_CNN_model_" + f"{(100*correct):>0.1f}%"
)





import time
model_name = f"{(100*correct):>0.1f}%--" + time.strftime("%Y-%m-%d_%H-%M-%S") + "_CNN_model.pth"
torch.save(CNN_model.state_dict(), MODELS_DIR / model_name)
print("Saved PyTorch Model State to " +  model_name)



